{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LexRankサンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_rank(sentences, n, t = 0.1):\n",
    "    \"\"\"\n",
    "    LexRankで文章を要約する．\n",
    "    @param  sentences: list\n",
    "        文章([[w1,w2,w3],[w1,w3,w4,w5],..]のような文リスト)\n",
    "    @param  n: int\n",
    "        文章に含まれる文の数\n",
    "    @param  t: float\n",
    "        コサイン類似度の閾値(default 0.1)\n",
    "    @return : list\n",
    "        LexRank\n",
    "    \"\"\"\n",
    "    cosine_matrix = numpy.zeros((n, n))\n",
    "    degrees = numpy.zeros((n,))\n",
    "    l = []\n",
    "\n",
    "     # 1. 隣接行列の作成\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            cosine_matrix[i][j] = idf_modified_cosine(sentences, sentences[i], sentences[j])\n",
    "            if cosine_matrix[i][j] > t:\n",
    "                cosine_matrix[i][j] = 1\n",
    "                degrees[i] += 1\n",
    "            else:\n",
    "                cosine_matrix[i][j] = 0\n",
    "\n",
    "    # 2.LexRank計算\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            cosine_matrix[i][j] = cosine_matrix[i][j] / degrees[i]\n",
    "\n",
    "    ratings = power_method(cosine_matrix, n, t)\n",
    "\n",
    "    return zip(sentences, ratings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_modified_cosine(sentences, sentence1, sentence2):\n",
    "    \"\"\"\n",
    "    2文間のコサイン類似度を計算\n",
    "    @param  sentence1: list\n",
    "        文1([w1,w2,w3]のような単語リスト)\n",
    "    @param  sentence2: list\n",
    "        文2([w1,w2,w3]のような単語リスト)\n",
    "    @param  sentences: list\n",
    "        文章([[w1,w2,w3],[w1,w3,w4,w5],..]のような単語リスト)\n",
    "    @return : float\n",
    "        コサイン類似度\n",
    "    \"\"\"\n",
    "    tf1 = compute_tf(sentence1)\n",
    "    tf2 = compute_tf(sentence2)\n",
    "    idf_metrics = compute_idf(sentences)\n",
    "    return cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def compute_tf(sentence):\n",
    "    \"\"\"\n",
    "    TFを計算\n",
    "    @param  sentence: list\n",
    "        文([w1,w2,w3]のような単語リスト)\n",
    "    @return : list\n",
    "        TFリスト\n",
    "    \"\"\"\n",
    "    tf_values = Counter(sentence)\n",
    "    tf_metrics = {}\n",
    "\n",
    "    max_tf = find_tf_max(tf_values)\n",
    "\n",
    "    for term, tf in tf_values.items():\n",
    "        tf_metrics[term] = tf / max_tf\n",
    "\n",
    "    return tf_metrics\n",
    "\n",
    "\n",
    "def find_tf_max(terms):\n",
    "    \"\"\"\n",
    "    単語の最大出現頻度を探索\n",
    "    @param  terms: list\n",
    "        単語の出現頻度\n",
    "    @return : int\n",
    "        単語の最大出現頻度\n",
    "    \"\"\"\n",
    "    return max(terms.values()) if terms else 1\n",
    "\n",
    "\n",
    "def compute_idf(sentences):\n",
    "    \"\"\"\n",
    "    文章中の単語のIDF値を計算\n",
    "    @param sentences: list\n",
    "        文章([[w1,w2,w3],[w1,w3,w4,w5],..]のような単語リスト)\n",
    "    @return: list\n",
    "        IDFリスト\n",
    "    \"\"\"\n",
    "    idf_metrics = {}\n",
    "    sentences_count = len(sentences)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for term in sentence:\n",
    "            if term not in idf_metrics:\n",
    "                n_j = sum(1 for s in sentences if term in s)\n",
    "                idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n",
    "\n",
    "    return idf_metrics\n",
    "\n",
    "\n",
    "def cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics):\n",
    "    \"\"\"\n",
    "    コサイン類似度を計算\n",
    "    @param  sentence1: list\n",
    "        文1([w1,w2,w3]のような単語リスト)\n",
    "    @param  sentence2: list\n",
    "        文2([w1,w2,w3]のような単語リスト)\n",
    "    @param  tf1: list\n",
    "        文1のTFリスト\n",
    "    @param  tf2: list\n",
    "        文2のTFリスト\n",
    "    @param  idf_metrics: list\n",
    "        文章のIDFリスト\n",
    "    @return : float\n",
    "        コサイン類似度\n",
    "    \"\"\"\n",
    "    unique_words1 = set(sentence1)\n",
    "    unique_words2 = set(sentence2)\n",
    "    common_words = unique_words1 & unique_words2\n",
    "\n",
    "    numerator = sum((tf1[t] * tf2[t] * idf_metrics[t] ** 2) for t in common_words)\n",
    "    denominator1 = sum((tf1[t] * idf_metrics[t]) ** 2 for t in unique_words1)\n",
    "    denominator2 = sum((tf2[t] * idf_metrics[t]) ** 2 for t in unique_words2)\n",
    "\n",
    "    if denominator1 > 0 and denominator2 > 0:\n",
    "        return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n",
    "    else:\n",
    "        return 0.0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method(cosine_matrix, n, e):\n",
    "    \"\"\"\n",
    "    べき乗法を行なう\n",
    "    @param  scosine_matrix: list\n",
    "        確率行列\n",
    "    @param  n: int\n",
    "        文章中の文の数\n",
    "    @param  e: float\n",
    "        許容誤差ε\n",
    "    @return: list\n",
    "        LexRank\n",
    "    \"\"\"\n",
    "    transposed_matrix = cosine_matrix.T\n",
    "    sentences_count = n\n",
    "\n",
    "    p_vector = numpy.array([1.0 / sentences_count] * sentences_count)\n",
    "\n",
    "    lambda_val = 1.0\n",
    "\n",
    "    while lambda_val > e:\n",
    "        next_p = numpy.dot(transposed_matrix, p_vector)\n",
    "        lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))\n",
    "        p_vector = next_p\n",
    "\n",
    "    return p_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 試す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "text = ''\n",
    "files = glob.glob('./data/*.txt')\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        text += f.read()\n",
    "documents = text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分かち書き\n",
    "import MeCab\n",
    "tagger = MeCab.Tagger('-Owakati')\n",
    "def wakati(tagger, document):\n",
    "     return tagger.parse(document).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(map(lambda doc: wakati(tagger, doc), documents))\n",
    "sentences = list(filter(lambda s: len(s) > 0, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = lex_rank(sentences, len(sentences), 0.1)\n",
    "scores = []\n",
    "for (sentence, rating) in result:\n",
    "    scores.append({\"score\": rating, \"sentence\": ''.join(sentence)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "そのアルゴリズムは、第一にそのデータが生成した潜在的機構の特徴を捉え、複雑な関係を識別（すなわち定量化）する。第二にその識別したパターンを用いて、新たなデータについて予測を行う。データは、観測された変数群のとりうる関係の具体例と見ることができる。一方、アルゴリズムは、機械学習者として観測されたデータの部分（訓練例などと呼ぶ）を学習することで、データに潜在する確率分布の特徴を捉え、学習によって得た知識を用いて、新たな入力データについて知的な決定を行う。\n",
      "機械学習システムによっては、人間の直観によるデータ解析の必要性を排除しようとしているが、人間と機械の協調的相互作用を取り入れたものもある。しかし、そもそもシステムのデータ表現方法やデータの特徴を探る機構は、人間が設計したものであり、人間の直観を完全に排除することはできない。\n",
      "という例外はあるが、基本的に学会も学術誌も別々である。それらの間の混同の最大の原因は、それらの基本的前提に由来する。機械学習では、既知の知識を再生成できるかどうかで性能を評価するが、データマイニングではそれまで「未知」だった知識を発見することが重視される。したがって、既知の知識によって評価するなら「教師なしの技法」よりも「教師ありの技法」の方が容易に優れた結果を示すことができる。しかし、典型的なデータマイニングでは、訓練データが用意できないので、「教師ありの技法」を採用することができない。\n"
     ]
    }
   ],
   "source": [
    "# 全文のスコアができたので、そのレートを高い順に出すdict化してレートソート？\n",
    "sorted_score = sorted(list(map(lambda s: s[\"score\"], scores)), reverse = True)\n",
    "limit = sorted_score[2]\n",
    "for s in scores:\n",
    "    if(s[\"score\"] < limit):\n",
    "        continue\n",
    "    print(s[\"sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
