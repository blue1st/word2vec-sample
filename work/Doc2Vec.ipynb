{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import codecs\n",
    "from pprint import pprint\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "from gensim.models import doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "tagger = MeCab.Tagger('-Owakati')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "file = './data/sample-input.txt'\n",
    "sentences = []\n",
    "with open(file, 'r') as f:\n",
    "    input_text = f.read()\n",
    "    input_text = input_text.replace('。', \"。\\n\").split(\"\\n\")\n",
    "    listed =  list(filter(lambda row: len(row) > 0, input_text))\n",
    "    sentences = list(map(lambda text: tagger.parse(text).split(' '), listed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for i, words in enumerate(sentences):\n",
    "    ls.append(doc2vec.TaggedDocument(words=words, tags=[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-21 22:03:17,280 : INFO : collecting all words and their counts\n",
      "2018-01-21 22:03:17,283 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-01-21 22:03:17,284 : INFO : collected 394 word types and 64 unique tags from a corpus of 64 examples and 1532 words\n",
      "2018-01-21 22:03:17,285 : INFO : Loading a fresh vocabulary\n",
      "2018-01-21 22:03:17,289 : INFO : min_count=1 retains 394 unique words (100% of original 394, drops 0)\n",
      "2018-01-21 22:03:17,291 : INFO : min_count=1 leaves 1532 word corpus (100% of original 1532, drops 0)\n",
      "2018-01-21 22:03:17,295 : INFO : deleting the raw counts dictionary of 394 items\n",
      "2018-01-21 22:03:17,297 : INFO : sample=1e-06 downsamples 394 most-common words\n",
      "2018-01-21 22:03:17,298 : INFO : downsampling leaves estimated 25 word corpus (1.6% of prior 1532)\n",
      "2018-01-21 22:03:17,300 : INFO : estimated required memory for 394 words and 300 dimensions: 1219400 bytes\n",
      "2018-01-21 22:03:17,303 : INFO : resetting layer weights\n",
      "2018-01-21 22:03:17,318 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,327 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,328 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,329 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,331 : INFO : training on 7660 raw words (454 effective words) took 0.0s, 47190 effective words/s\n",
      "2018-01-21 22:03:17,332 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,334 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,353 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,354 : INFO : training on 7660 raw words (446 effective words) took 0.0s, 32665 effective words/s\n",
      "2018-01-21 22:03:17,355 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,357 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,359 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,372 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,374 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,380 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,382 : INFO : training on 7660 raw words (438 effective words) took 0.0s, 30016 effective words/s\n",
      "2018-01-21 22:03:17,383 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,384 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,386 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,391 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,396 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,405 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,407 : INFO : training on 7660 raw words (436 effective words) took 0.0s, 26639 effective words/s\n",
      "2018-01-21 22:03:17,410 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,412 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,415 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,424 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,426 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,434 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,435 : INFO : training on 7660 raw words (439 effective words) took 0.0s, 30181 effective words/s\n",
      "2018-01-21 22:03:17,436 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,437 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,439 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,445 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,451 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,459 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,461 : INFO : training on 7660 raw words (447 effective words) took 0.0s, 26594 effective words/s\n",
      "2018-01-21 22:03:17,462 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,465 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,466 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,482 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,485 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,488 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,489 : INFO : training on 7660 raw words (454 effective words) took 0.0s, 28466 effective words/s\n",
      "2018-01-21 22:03:17,492 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,494 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,495 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,511 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,513 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,516 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,518 : INFO : training on 7660 raw words (448 effective words) took 0.0s, 25181 effective words/s\n",
      "2018-01-21 22:03:17,520 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,522 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,524 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "訓練開始\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-21 22:03:17,542 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,547 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,556 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,558 : INFO : training on 7660 raw words (433 effective words) took 0.0s, 15443 effective words/s\n",
      "2018-01-21 22:03:17,562 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,563 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,566 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,576 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,578 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,586 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,587 : INFO : training on 7660 raw words (454 effective words) took 0.0s, 39306 effective words/s\n",
      "2018-01-21 22:03:17,588 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,589 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,590 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,605 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,607 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,608 : INFO : training on 7660 raw words (452 effective words) took 0.0s, 47559 effective words/s\n",
      "2018-01-21 22:03:17,609 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,610 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,611 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,622 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,635 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,636 : INFO : training on 7660 raw words (437 effective words) took 0.0s, 22719 effective words/s\n",
      "2018-01-21 22:03:17,638 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,639 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,641 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,663 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,669 : INFO : training on 7660 raw words (448 effective words) took 0.0s, 20514 effective words/s\n",
      "2018-01-21 22:03:17,671 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,673 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,675 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,699 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,700 : INFO : training on 7660 raw words (461 effective words) took 0.0s, 21306 effective words/s\n",
      "2018-01-21 22:03:17,704 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,705 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,707 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,719 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,729 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,730 : INFO : training on 7660 raw words (426 effective words) took 0.0s, 24305 effective words/s\n",
      "2018-01-21 22:03:17,731 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,732 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,733 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,747 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,752 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,753 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,754 : INFO : training on 7660 raw words (453 effective words) took 0.0s, 31315 effective words/s\n",
      "2018-01-21 22:03:17,755 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,757 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,758 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-21 22:03:17,769 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,778 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,782 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,783 : INFO : training on 7660 raw words (428 effective words) took 0.0s, 23031 effective words/s\n",
      "2018-01-21 22:03:17,785 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,786 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,788 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,799 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,802 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,813 : INFO : training on 7660 raw words (457 effective words) took 0.0s, 23266 effective words/s\n",
      "2018-01-21 22:03:17,814 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,816 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,817 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,828 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,833 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,838 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,840 : INFO : training on 7660 raw words (449 effective words) took 0.0s, 24433 effective words/s\n",
      "2018-01-21 22:03:17,841 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,844 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,845 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,851 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,854 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,866 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,868 : INFO : training on 7660 raw words (436 effective words) took 0.0s, 23750 effective words/s\n",
      "2018-01-21 22:03:17,869 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,870 : WARNING : supplied example count (320) did not equal expected count (50)\n",
      "2018-01-21 22:03:17,871 : INFO : training model with 3 workers on 394 vocabulary and 300 features, using sg=1 hs=0 sample=1e-06 negative=5 window=15\n",
      "2018-01-21 22:03:17,883 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-21 22:03:17,886 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-21 22:03:17,895 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-21 22:03:17,896 : INFO : training on 7660 raw words (463 effective words) took 0.0s, 24964 effective words/s\n",
      "2018-01-21 22:03:17,896 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-01-21 22:03:17,897 : WARNING : supplied example count (320) did not equal expected count (50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n"
     ]
    }
   ],
   "source": [
    "# https://deepage.net/machine_learning/2017/01/08/doc2vec.html\n",
    "model = doc2vec.Doc2Vec(ls, dm=0, size=300, window=15, alpha=.025,\n",
    "        min_alpha=.025, min_count=1, sample=1e-6)\n",
    "\n",
    "print('\\n訓練開始')\n",
    "for epoch in range(20):\n",
    "    print('Epoch: {}'.format(epoch + 1))\n",
    "    model.train(ls, total_examples = 10, epochs = model.iter)\n",
    "    model.alpha -= (0.025 - 0.0001) / 19\n",
    "    model.min_alpha = model.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-21 22:03:18,537 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 0.16756100952625275), (37, 0.1674322932958603), (28, 0.14117373526096344)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.docvecs.most_similar(0, topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "機械学習（きかいがくしゅう、英:machinelearning）とは、人工知能における研究課題の一つで、人間が自然に行っている学習能力と同様の機能をコンピュータで実現しようとする技術・手法のことである。\n",
      "\n",
      "センサやデータベースなどから、ある程度の数のサンプルデータ集合を入力して解析を行い、そのデータから有用な規則、ルール、知識表現、判断基準などを抽出し、アルゴリズムを発展させる。\n",
      "\n",
      "しかし、典型的なデータマイニングでは、訓練データが用意できないので、「教師ありの技法」を採用することができない。\n",
      "\n",
      "この2つは、さまざまな面でオーバーラップしている。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(''.join(ls[0].words))\n",
    "print(''.join(ls[2].words))\n",
    "print(''.join(ls[37].words))\n",
    "print(''.join(ls[28].words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
